{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c247a53",
   "metadata": {},
   "source": [
    "# example script to try to train and eval BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097281f5",
   "metadata": {},
   "source": [
    "## 1. eval pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72177fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp==3.7.4.post0\n",
      "argon2-cffi==21.1.0\n",
      "async-timeout==3.0.1\n",
      "attrs==21.2.0\n",
      "backcall==0.2.0\n",
      "bleach==4.1.0\n",
      "certifi==2021.10.8\n",
      "cffi==1.15.0\n",
      "chardet==4.0.0\n",
      "charset-normalizer==2.0.7\n",
      "click==8.0.3\n",
      "colorama==0.4.4\n",
      "comet-ml==3.19.0\n",
      "configobj==5.0.6\n",
      "datasets==1.13.3\n",
      "debugpy==1.5.0\n",
      "decorator==5.1.0\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.4\n",
      "dulwich==0.20.25\n",
      "entrypoints==0.3\n",
      "everett==2.0.1\n",
      "filelock==3.3.1\n",
      "fsspec==2021.10.1\n",
      "huggingface-hub==0.0.19\n",
      "idna==3.3\n",
      "ipykernel==6.4.1\n",
      "ipython==7.28.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.6.5\n",
      "jedi==0.18.0\n",
      "Jinja2==3.0.2\n",
      "joblib==1.1.0\n",
      "jsonschema==4.1.0\n",
      "jupyter-client==7.0.6\n",
      "jupyter-core==4.8.1\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-widgets==1.0.2\n",
      "MarkupSafe==2.0.1\n",
      "matplotlib-inline==0.1.3\n",
      "mistune==0.8.4\n",
      "multidict==5.2.0\n",
      "multiprocess==0.70.12.2\n",
      "nbclient==0.5.4\n",
      "nbconvert==6.2.0\n",
      "nbformat==5.1.3\n",
      "nest-asyncio==1.5.1\n",
      "notebook==6.4.4\n",
      "numpy==1.21.2\n",
      "nvidia-ml-py3==7.352.0\n",
      "packaging==21.0\n",
      "pandas==1.3.3\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.2\n",
      "pickleshare==0.7.5\n",
      "Pillow==8.4.0\n",
      "portalocker==2.3.2\n",
      "prometheus-client==0.11.0\n",
      "prompt-toolkit==3.0.20\n",
      "pyarrow==5.0.0\n",
      "pycparser==2.20\n",
      "Pygments==2.10.0\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.18.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2021.3\n",
      "pywin32==302\n",
      "pywinpty==1.1.4\n",
      "PyYAML==6.0\n",
      "pyzmq==22.3.0\n",
      "regex==2021.10.8\n",
      "requests==2.26.0\n",
      "requests-toolbelt==0.9.1\n",
      "sacrebleu==2.0.0\n",
      "sacremoses==0.0.46\n",
      "semantic-version==2.8.5\n",
      "Send2Trash==1.8.0\n",
      "sentencepiece==0.1.96\n",
      "six==1.16.0\n",
      "tabulate==0.8.9\n",
      "terminado==0.12.1\n",
      "testpath==0.5.0\n",
      "tokenizers==0.10.3\n",
      "torch==1.9.1+cu102\n",
      "torchaudio==0.9.1\n",
      "torchvision==0.10.1+cu102\n",
      "tornado==6.1\n",
      "tqdm==4.62.3\n",
      "traitlets==5.1.0\n",
      "transformers==4.11.3\n",
      "typing-extensions==3.10.0.2\n",
      "urllib3==1.26.7\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.2.1\n",
      "widgetsnbextension==3.5.1\n",
      "wincertstore==0.2\n",
      "wrapt==1.12.1\n",
      "wurlitzer==3.0.2\n",
      "xxhash==2.0.2\n",
      "yarl==1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905add05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5734c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-sk\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36a5327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-sk', vocab_size=60025, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6128e14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianConfig {\n",
       "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-sk\",\n",
       "  \"_num_labels\": 3,\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"swish\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      60024\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 60024,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"marian\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 60024,\n",
       "  \"scale_embedding\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 60025\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f34d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(60025, 512, padding_idx=60024)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(60025, 512, padding_idx=60024)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(60025, 512, padding_idx=60024)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=60025, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "marian = MarianMTModel(model.config)\n",
    "marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8796184",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bfc30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-sk-lang1=en,lang2=sk\n",
      "Reusing dataset open_subtitles (C:\\Users\\marek\\.cache\\huggingface\\datasets\\open_subtitles\\en-sk-lang1=en,lang2=sk\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open_subtitles\", lang1=\"en\",lang2=\"sk\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8b6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import sentencepiece as spm\n",
    "\n",
    "# # spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000, user_defined_symbols=['foo', 'bar'])\n",
    "\n",
    "# batch_size=50\n",
    "# def batch_iterator(batch_size):\n",
    "#     for i in range(0, len(dataset), batch_size):\n",
    "#         print(dataset[i : i + batch_size]['translation'])\n",
    "#         yield dataset[i : i + batch_size][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eddabd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(batch_iterator(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58647842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cced628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marian_tokenizer = MarianTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd1b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-sk-lang1=en,lang2=sk\n",
      "Reusing dataset open_subtitles (C:\\Users\\marek\\.cache\\huggingface\\datasets\\open_subtitles\\en-sk-lang1=en,lang2=sk\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4130c872b8864bd19e676e16b7dc7a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open_subtitles\", lang1=\"en\",lang2=\"sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c74e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'meta', 'translation'],\n",
       "        num_rows: 8850871\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35e7182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'meta': {'year': Value(dtype='uint32', id=None),\n",
       "  'imdbId': Value(dtype='uint32', id=None),\n",
       "  'subtitleId': {'en': Value(dtype='uint32', id=None),\n",
       "   'sk': Value(dtype='uint32', id=None)},\n",
       "  'sentenceIds': {'en': Sequence(feature=Value(dtype='uint32', id=None), length=-1, id=None),\n",
       "   'sk': Sequence(feature=Value(dtype='uint32', id=None), length=-1, id=None)}},\n",
       " 'translation': Translation(languages=['en', 'sk'], id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61cdcadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 512\n",
    "source_lang = \"en\"\n",
    "target_lang = \"sk\"\n",
    "prefix = \"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e13058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6716f4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'meta', 'translation'],\n",
       "        num_rows: 8850871\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    dataset = pickle.load(open(\"tokenized_datasets.pickle\", \"rb\"))\n",
    "except (OSError, IOError) as e:\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    pickle.dump(dataset, open(\"tokenized_datasets.pickle\", \"wb\"))\n",
    "\n",
    "tokenized_datasets = dataset\n",
    "tokenized_datasets\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec3ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\marek\\.cache\\huggingface\\datasets\\open_subtitles\\en-sk-lang1=en,lang2=sk\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198\\cache-1b05abe45041b0e0.arrow and C:\\Users\\marek\\.cache\\huggingface\\datasets\\open_subtitles\\en-sk-lang1=en,lang2=sk\\0.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198\\cache-4ba7b61841ebaf20.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = tokenized_datasets['train'].train_test_split(test_size=0.2,seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb3f9877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'meta', 'translation'],\n",
       "        num_rows: 7080696\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'meta', 'translation'],\n",
       "        num_rows: 1770175\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4afb592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=marian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61995c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 1\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"Training-Helsinki-NLP/opus-mt-en-sk\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    no_cuda=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"comet_ml\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7e40804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bf0f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "#     print(preds[0])\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d5400f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    marian,\n",
    "    training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cd47470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316000).\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: meta, id, translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7080696\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7080696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 316000\n",
      "  Will skip the first 0 epochs then the first 316000 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d8d6ceeb14867a15fdb6ed2fdaf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/marekninja/huggingface/a0947d7b9bcf49feb0f53767e1ad3665\n",
      "\n",
      "Automatic Comet.ml online logging enabled\n",
      "Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1798353' max='7080696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1798353/7080696 32:40:54 < 116:27:40, 12.60 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177105' max='1770175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 177105/1770175 6:53:17 < 61:57:35, 7.14 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-316500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-315000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-315500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-317500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-316000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-316500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-318500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-317000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-317500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-319500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-318000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-318500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-320500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-319000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-319500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-321500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-320000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-320500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-322500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-321000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-321500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-323500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-322000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-322500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-324500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-323000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-323500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-325500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-324000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-324500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-326500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-325000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-325500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-327500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-326000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-326500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-328500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-327000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-327500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-329500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-328000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-328500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-330500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-329000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-329500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-331500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-330000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-330500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-332500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-331000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-331500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-333500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-332000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-332500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-334500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-333000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-333500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-335500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-334000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-334500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-336500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-335000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-335500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-337500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-336000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-336500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-338500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-337000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-337500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-339500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-338000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-338500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-340500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-339000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-339500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-341500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-340000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-340500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-342500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-341000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-341500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-343500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-342000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-342500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-344500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-343000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-343500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-345500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-344000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-344500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-346500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-345000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-345500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-347500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-346000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-346500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-348500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-347000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-347500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-349500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-348000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-348500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-350500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-349000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-349500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-351500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-350000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-350500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-352500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-351000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-351500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-353500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-352000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-352500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-354500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-353000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-353500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-355500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-354000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-354500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-356500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-355000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-355500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-357500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-356000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-356500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-358500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-357000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-357500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-359500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-358000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-358500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-360500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-359000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-359500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-361500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-360000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-360500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-362500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-361000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-361500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-363500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-362000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-362500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-364500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-363000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-363500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-365500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-364000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-364500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-366500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-365000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-365500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-367500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-366000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-366500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-368500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-367000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-367500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-369500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-368000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-368500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-370500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-369000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-369500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-371500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-370000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-370500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-372500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-371000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-371500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-373500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-372000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-372500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-374500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-373000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-373500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-375500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-374000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-374500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-376500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-375000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-375500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-377500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-376000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-376500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-378500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-377000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-377500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-379500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-378000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-378500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-380500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-379000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-379500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-381500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-380000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-380500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-382500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-381000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-381500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-383500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-382000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-382500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-384500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-383000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-383500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-385500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-384000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-384500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-386500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-385000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-385500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-387500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-386000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-386500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-388500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-387000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-387500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-389500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-388000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-388500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-390500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-389000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-389500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-391500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-390000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-390500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-392500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-391000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-391500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-393500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-392000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-392500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-394500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-393000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-393500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-395500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-394000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-394500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-396500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-395000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-395500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-397500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-396000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-396500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-398500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-397000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-397500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-399500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-398000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-398500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-400500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-399000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-399500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-401500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-400000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-400500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-402500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-401000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-401500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-403500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-402000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-402500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-404500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-403000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-403500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-405500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-404000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-404500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-406500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-405000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-405500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-407500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-406000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-406500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-408500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-407000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-407500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-409500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-408000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-408500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-410500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-409000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-409500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-411500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-410000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-410500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-412500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-411000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-411500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-413500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-412000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-412500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-414500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-413000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-413500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-415500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-414000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-414500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-416500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-415000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-415500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-417500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-416000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-416500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-418500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-417000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-417500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-419500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-418000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-418500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-420500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-419000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-419500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-421500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-420000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-420500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-422500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-421000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-421500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-423500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-422000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-422500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-424500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-423000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-423500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-425500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-424000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-424500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-426500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-425000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-425500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-427500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-426000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-426500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-428500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-427000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-427500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-429500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-428000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-428500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-430500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-429000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-429500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-431500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-430000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-430500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-432500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-431000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-431500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-433500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-432000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-432500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-434500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-433000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-433500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-435500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-434000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-434500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-436500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-435000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-435500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-437500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-436000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-436500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-438500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-437000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-437500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-439500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-438000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-438500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-440500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-439000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-439500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-441500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-440000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-440500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-442500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-441000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-441500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-443500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-442000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-442500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-444500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-443000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-443500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-445500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-444000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-444500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-446500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-445000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-445500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-447500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-446000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-446500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-448500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-447000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-447500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-449500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-448000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-448500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-450500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-449000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-449500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-451500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-450000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-450500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-452500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-451000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-451500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-453500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-452000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-452500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-454500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-453000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-453500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-455500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-454000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-454500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-456500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-455000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-455500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-457500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-456000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-456500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-458500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-457000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-457500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-459500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-458000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-458500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-460500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-459000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-459500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-461500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-460000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-460500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-462500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-461000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-461500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-463500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-462000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-462500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-464500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-463000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-463500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-465500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-464000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-464500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-466500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-465000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-465500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-467500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-466000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-466500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-468500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-467000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-467500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-469500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-468000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-468500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-470500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-469000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-469500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-471500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-470000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-470500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-472500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-471000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-471500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-473500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-472000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-472500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-474500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-473000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-473500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-475500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-474000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-474500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-476500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-475000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-475500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-477500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-476000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-476500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-478500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-477000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-477500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-479500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-478000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-478500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-480500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-479000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-479500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-481500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-480000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-480500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-482500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-481000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-481500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-483500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-482000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-482500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-484500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-483000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-483500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-485500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-484000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-484500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-486500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-485000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-485500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-487500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-486000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-486500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-488500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-487000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-487500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-489500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-488000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-488500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-490500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-489000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-489500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-491500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-490000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-490500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-492500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-491000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-491500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-493500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-492000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-492500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-494500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-493000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-493500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-495500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-494000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-494500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-496500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-495000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-495500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-497500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-496000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-496500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-498500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-497000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-497500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-499500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-498000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-498500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-500500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-499000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-499500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-501500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-500000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-500500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-502500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-501000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-501500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-503500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-502000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-502500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-504500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-503000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-503500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-505500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-504000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-504500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-506500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-505000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-505500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-507500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-506000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-506500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-508500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-507000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-507500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-509500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-508000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-508500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-510500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-509000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-509500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-511500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-510000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-510500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-512500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-511000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-511500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-513500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-512000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-512500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-514500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-513000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-513500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-515500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-514000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-514500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-516500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-515000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-515500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-517500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-516000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-516500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-518500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-517000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-517500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-519500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-518000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-518500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-520500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-519000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-519500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-521500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-520000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-520500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-522500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-521000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-521500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-523500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-522000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-522500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-524500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-523000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-523500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-525500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-524000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-524500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-526500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-525000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-525500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-527500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-526000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-526500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-528500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-527000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-527500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-529500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-528000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-528500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-530500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-529000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-529500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-531500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-530000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-530500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-532500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-531000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-531500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-533500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-532000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-532500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-534500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-533000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-533500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-535500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-534000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-534500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-536500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-535000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-535500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-537500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-536000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-536500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-538500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-537000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-537500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-539500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-538000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-538500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-540500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-539000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-539500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-541500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-540000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-540500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-542500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-541000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-541500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-543500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-542000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-542500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-544500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-543000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-543500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-545500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-544000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-544500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-546500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-545000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-545500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-547500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-546000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-546500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-548500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-547000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-547500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-549500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-548000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-548500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-550500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-549000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-549500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-551500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-550000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-550500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-552500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-551000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-551500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-553500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-552000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-552500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-554500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-553000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-553500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-555500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-554000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-554500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-556500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-555000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-555500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-557500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-556000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-556500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-558500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-557000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-557500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-559500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-558000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-558500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-560500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-559000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-559500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-561500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-560000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-560500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-562500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-561000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-561500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-563500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-562000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-562500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-564500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-563000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-563500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-565500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-564000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-564500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-566500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-565000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-565500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-567500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-566000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-566500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-568500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-567000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-567500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-569500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-568000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-568500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-570500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-569000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-569500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-571500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-570000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-570500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-572500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-571000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-571500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-573500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-572000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-572500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-574500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-573000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-573500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-575500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-574000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-574500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-576500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-575000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-575500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-577500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-576000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-576500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-578500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-577000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-577500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-579500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-578000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-578500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-580500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-579000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-579500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-581500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-580000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-580500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-582500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-581000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-581500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-583500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-582000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-582500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-584500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-583000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-583500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-585500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-584000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-584500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-586500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-585000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-585500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-587500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-586000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-586500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-588500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-587000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-587500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-589500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-588000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-588500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-590500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-589000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-589500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-591500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-590000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-590500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-592500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-591000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-591500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-593500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-592000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-592500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-594500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-593000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-593500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-595500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-594000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-594500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-596500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-595000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-595500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-597500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-596000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-596500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-598500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-597000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-597500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-599500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-598000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-598500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-600500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-599000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-599500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-601500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-600000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-600500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-602500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-601000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-601500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-603500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-602000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-602500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-604500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-603000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-603500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-605500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-604000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-604500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-606500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-605000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-605500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-607500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-606000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-606500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-608500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-607000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-607500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-609500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-608000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-608500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-610500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-609000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-609500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-611500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-610000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-610500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-612500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-611000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-611500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-613500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-612000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-612500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-614500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-613000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-613500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-615500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-614000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-614500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-616500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-615000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-615500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-617500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-616000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-616500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-618500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-617000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-617500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-619500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-618000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-618500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-620500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-619000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-619500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-621500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-620000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-620500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-622500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-621000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-621500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-623500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-622000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-622500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-624500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-623000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-623500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-625500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-624000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-624500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-626500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-625000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-625500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-627500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-626000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-626500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-628500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-627000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-627500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-629500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-628000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-628500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-630500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-629000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-629500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-631500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-630000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-630500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-632500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-631000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-631500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-633500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-632000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-632500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-634500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-633000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-633500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-635500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-634000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-634500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-636500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-635000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-635500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-637500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-636000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-636500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-638500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-637000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-637500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-639500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-638000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-638500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-640500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-639000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-639500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-641500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-640000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-640500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-642500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-641000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-641500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-643500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-642000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-642500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-644500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-643000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-643500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-645500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-644000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-644500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-646500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-645000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-645500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-647500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-646000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-646500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-648500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-647000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-647500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-649500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-648000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-648500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-650500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-649000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-649500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-651500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-650000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-650500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-652500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-651000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-651500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-653500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-652000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-652500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-654500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-653000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-653500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-655500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-654000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-654500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-656500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-655000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-655500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-657500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-656000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-656500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-658500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-657000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-657500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-659500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-658000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-658500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-660500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-659000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-659500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-661500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-660000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-660500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-662500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-661000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-661500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-663500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-662000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-662500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-664500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-663000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-663500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-665500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-664000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-664500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-666500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-665000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-665500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-667500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-666000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-666500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-668500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-667000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-667500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-669500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-668000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-668500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-670500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-669000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-669500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-671500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-670000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-670500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-672500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-671000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-671500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-673500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-672000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-672500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-674500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-673000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-673500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-675500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-674000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-674500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-676500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-675000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-675500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-677500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-676000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-676500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-678500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-677000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-677500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-679500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-678000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-678500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-680500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-679000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-679500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-681500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-680000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-680500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-682500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-681000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-681500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-683500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-682000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-682500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-684500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-683000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-683500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-685500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-684000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-684500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-686500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-685000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-685500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-687500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-686000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-686500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-688500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-687000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-687500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-689500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-688000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-688500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-690500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-689000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-689500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-691500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-690000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-690500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-692500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-691000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-691500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-693500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-692000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-692500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-694500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-693000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-693500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-695500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-694000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-694500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-696500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-695000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-695500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-697500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-696000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-696500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-698500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-697000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-697500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-699500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-698000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-698500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-700500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-699000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-699500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-701500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-700000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-700500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-702500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-701000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-701500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-703500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-702000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-702500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-704500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-703000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-703500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-705500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-704000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-704500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-706500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-705000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-705500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-707500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-706000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-706500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-708500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-707000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-707500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-709500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-708000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-708500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-710500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-709000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-709500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-711500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-710000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-710500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-712500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-711000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-711500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-713500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-712000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-712500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-714500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-713000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-713500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-715500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-714000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-714500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-716500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-715000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-715500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-717500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-716000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-716500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-718500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-717000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-717500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-719500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-718000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-718500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-720500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-719000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-719500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-721500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-720000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-720500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-722500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-721000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-721500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-723500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-722000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-722500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-724500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-723000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-723500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-725500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-724000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-724500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-726500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-725000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-725500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-727500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-726000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-726500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-728500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-727000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-727500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-729500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-728000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-728500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-730500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-729000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-729500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-731500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-730000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-730500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-732500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-731000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-731500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-733500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-732000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-732500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-734500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-733000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-733500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-735500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-734000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-734500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-736500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-735000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-735500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-737500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-736000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-736500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-738500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-737000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-737500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-739500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-738000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-738500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-740500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-739000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-739500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-741500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-740000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-740500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-742500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-741000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-741500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-743500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-742000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-742500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-744500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-743000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-743500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-745500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-744000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-744500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-746500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-745000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-745500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-747500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-746000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-746500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-748500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-747000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-747500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-749500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-748000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-748500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-750500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-749000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-749500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-751500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-750000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-750500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-752500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-751000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-751500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-753500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-752000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-752500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-754500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-753000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-753500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-755500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-754000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-754500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-756500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-755000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-755500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-757500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-756000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-756500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-758500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-757000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-757500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-759500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-758000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-758500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-760500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-759000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-759500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-761500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-760000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-760500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-762500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-761000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-761500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-763500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-762000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-762500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-764500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-763000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-763500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-765500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-764000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-764500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-766500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-765000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-765500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-767500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-766000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-766500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-768500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-767000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-767500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-769500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-768000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-768500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-770500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-769000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-769500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-771500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-770000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-770500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-772500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-771000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-771500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-773500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-772000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-772500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-774500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-773000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-773500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-775500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-774000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-774500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-776500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-775000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-775500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-777500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-776000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-776500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-778500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-777000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-777500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-779500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-778000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-778500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-780500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-779000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-779500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-781500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-780000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-780500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-782500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-781000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-781500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-783500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-782000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-782500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-784500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-783000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-783500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-785500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-784000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-784500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-786500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-785000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-785500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-787500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-786000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-786500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-788500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-787000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-787500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-789500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-788000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-788500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-790500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-789000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-789500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-791500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-790000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-790500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-792500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-791000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-791500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-793500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-792000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-792500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-794500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-793000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-793500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-795500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-794000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-794500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-796500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-795000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-795500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-797500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-796000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-796500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-798500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-797000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-797500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-799500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-798000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-798500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-800500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-799000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-799500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-801500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-800000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-800500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-802500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-801000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-801500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-803500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-802000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-802500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-804500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-803000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-803500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-805500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-804000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-804500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-806500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-805000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-805500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-807500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-806000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-806500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-808500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-807000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-807500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-809500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-808000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-808500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-810500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-809000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-809500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-811500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-810000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-810500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-812500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-811000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-811500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-813500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-812000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-812500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-814500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-813000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-813500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-815500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-814000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-814500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-816500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-815000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-815500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-817500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-816000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-816500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-818500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-817000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-817500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-819500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-818000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-818500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-820500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-819000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-819500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-821500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-820000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-820500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-822500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-821000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-821500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-823500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-822000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-822500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-824500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-823000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-823500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-825500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-824000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-824500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-826500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-825000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-825500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-827500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-826000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-826500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-828500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-827000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-827500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-829500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-828000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-828500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-830500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-829000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-829500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-831500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-830000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-830500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-832500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-831000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-831500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-833500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-832000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-832500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-834500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-833000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-833500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-835500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-834000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-834500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-836500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-835000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-835500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-837500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-836000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-836500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-838500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-837000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-837500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-839500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-838000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-838500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-840500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-839000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-839500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-841500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-840000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-840500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-842500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-841000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-841500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-843500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-842000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-842500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-844500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-843000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-843500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-845500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-844000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-844500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-846500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-845000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-845500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-847500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-846000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-846500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-848500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-847000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-847500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-849500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-848000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-848500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-850500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-849000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-849500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-851500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-850000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-850500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-852500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-851000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-851500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-853500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-852000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-852500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-854500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-853000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-853500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-855500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-854000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-854500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-856500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-855000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-855500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-857500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-856000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-856500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-858500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-857000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-857500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-859500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-858000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-858500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-860500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-859000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-859500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-861500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-860000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-860500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-862500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-861000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-861500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-863500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-862000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-862500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-864500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-863000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-863500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-865500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-864000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-864500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-866500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-865000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-865500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-867500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-866000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-866500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-868500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-867000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-867500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-869500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-868000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-868500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-870500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-869000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-869500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-871500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-870000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-870500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-872500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-871000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-871500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-873500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-872000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-872500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-874500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-873000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-873500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-875500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-874000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-874500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-876500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-875000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-875500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-877500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-876000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-876500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-878500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-877000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-877500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-879500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-878000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-878500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-880500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-879000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-879500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-881500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-880000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-880500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-882500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-881000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-881500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-883500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-882000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-882500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-884500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-883000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-883500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-885500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-884000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-884500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-886500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-885000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-885500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-887500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-886000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-886500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-888500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-887000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-887500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-889500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-888000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-888500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-890500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-889000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-889500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-891500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-890000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-890500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-892500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-891000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-891500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-893500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-892000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-892500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-894500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-893000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-893500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-895500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-894000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-894500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-896500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-895000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-895500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-897500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-896000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-896500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-898500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-897000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-897500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-899500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-898000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-898500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-900500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-899000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-899500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-901500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-900000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-900500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-902500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-901000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-901500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-903500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-902000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-902500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-904500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-903000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-903500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-905500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-904000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-904500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-906500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-905000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-905500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-907500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-906000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-906500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-908500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-907000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-907500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-909500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-908000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-908500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-910500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-909000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-909500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-911500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-910000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-910500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-912500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-911000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-911500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-913500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-912000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-912500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-914500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-913000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-913500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-915500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-914000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-914500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-916500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-915000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-915500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-917500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-916000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-916500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-918500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-917000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-917500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-919500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-918000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-918500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-920500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-919000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-919500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-921500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-920000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-920500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-922500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-921000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-921500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-923500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-922000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-922500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-924500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-923000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-923500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-925500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-924000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-924500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-926500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-925000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-925500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-927500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-926000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-926500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-928500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-927000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-927500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-929500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-928000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-928500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-930500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-929000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-929500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-931500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-930000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-930500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-932500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-931000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-931500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-933500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-932000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-932500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-934500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-933000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-933500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-935500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-934000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-934500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-936500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-935000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-935500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-937500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-936000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-936500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-938500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-937000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-937500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-939500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-938000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-938500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-940500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-939000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-939500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-941500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-940000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-940500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-942500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-941000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-941500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-943500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-942000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-942500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-944500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-943000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-943500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-945500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-944000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-944500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-946500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-945000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-945500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-947500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-946000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-946500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-948500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-947000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-947500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-949500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-948000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-948500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-950500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-949000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-949500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-951500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-950000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-950500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-952500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-951000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-951500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-953500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-952000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-952500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-954500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-953000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-953500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-955500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-954000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-954500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-956500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-955000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-955500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-957500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-956000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-956500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-958500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-957000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-957500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-959500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-958000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-958500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-960500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-959000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-959500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-961500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-960000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-960500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-962500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-961000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-961500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-963500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-962000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-962500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-964500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-963000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-963500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-965500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-964000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-964500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-966500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-965000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-965500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-967500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-966000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-966500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-968500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-967000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-967500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-969500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-968000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-968500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-970500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-969000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-969500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-971500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-970000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-970500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-972500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-971000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-971500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-973500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-972000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-972500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-974500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-973000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-973500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-975500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-974000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-974500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-976500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-975000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-975500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-977500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-976000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-976500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-978500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-977000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-977500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-979500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-978000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-978500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-980500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-979000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-979500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-981500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-980000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-980500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-982500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-981000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-981500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-983500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-982000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-982500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-984500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-983000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-983500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-985500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-984000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-984500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-986500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-985000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-985500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-987500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-986000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-986500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-988500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-987000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-987500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-989500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-988000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-988500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-990500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-989000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-989500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-991500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-990000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-990500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-992500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-991000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-991500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-993500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-992000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-992500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-994500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-993000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-993500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-995500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-994000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-994500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-996500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-995000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-995500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-997500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-996000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-996500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-998500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-997000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-997500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-999500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-998000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-998500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1000500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-999000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-999500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1001500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1000000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1000500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1002500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1001000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1001500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1003500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1002000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1002500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1004500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1003000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1003500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1005500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1004000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1004500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1006500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1005000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1005500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1007500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1006000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1006500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1008500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1007000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1007500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1009500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1008000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1008500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1010500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1009000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1009500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1011500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1010000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1010500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1012500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1011000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1011500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1013500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1012000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1012500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1014500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1013000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1013500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1015500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1014000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1014500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1016500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1015000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1015500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1017500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1016000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1016500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1018500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1017000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1017500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1019500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1018000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1018500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1020500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1019000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1019500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1021500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1020000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1020500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1022500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1021000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1021500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1023500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1022000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1022500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1024500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1023000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1023500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1025500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1024000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1024500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1026500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1025000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1025500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1027500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1026000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1026500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1028500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1027000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1027500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1029500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1028000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1028500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1030500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1029000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1029500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1031500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1030000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1030500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1032500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1031000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1031500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1033500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1032000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1032500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1034500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1033000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1033500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1035500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1034000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1034500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1036500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1035000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1035500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1037500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1036000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1036500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1038500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1037000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1037500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1039500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1038000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1038500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1040500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1039000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1039500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1041500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1040000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1040500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1042500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1041000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1041500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1043500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1042000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1042500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1044500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1043000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1043500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1045500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1044000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1044500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1046500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1045000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1045500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1047500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1046000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1046500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1048500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1047000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1047500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1049500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1048000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1048500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1050500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1049000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1049500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1051500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1050000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1050500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1052500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1051000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1051500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1053500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1052000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1052500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1054500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1053000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1053500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1055500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1054000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1054500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1056500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1055000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1055500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1057500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1056000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1056500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1058500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1057000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1057500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1059500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1058000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1058500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1060500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1059000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1059500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1061500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1060000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1060500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1062500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1061000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1061500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1063500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1062000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1062500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1064500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1063000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1063500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1065500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1064000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1064500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1066500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1065000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1065500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1067500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1066000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1066500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1068500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1067000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1067500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1069500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1068000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1068500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1070500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1069000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1069500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1071500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1070000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1070500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1072500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1071000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1071500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1073500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1072000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1072500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1074500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1073000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1073500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1075500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1074000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1074500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1076500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1075000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1075500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1077500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1076000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1076500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1078500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1077000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1077500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1079500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1078000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1078500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1080500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1079000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1079500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1081500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1080000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1080500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1082500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1081000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1081500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1083500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1082000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1082500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1084500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1083000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1083500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1085500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1084000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1084500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1086500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1085000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1085500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1087500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1086000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1086500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1088500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1087000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1087500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1089500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1088000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1088500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1090500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1089000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1089500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1091500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1090000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1090500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1092500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1091000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1091500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1093500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1092000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1092500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1094500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1093000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1093500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1095500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1094000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1094500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1096500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1095000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1095500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1097500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1096000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1096500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1098500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1097000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1097500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1099500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1098000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1098500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1100500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1099000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1099500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1101500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1100000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1100500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1102500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1101000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1101500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1103500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1102000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1102500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1104500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1103000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1103500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1105500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1104000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1104500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1106500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1105000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1105500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1107500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1106000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1106500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1108500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1107000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1107500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1109500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1108000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1108500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1110500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1109000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1109500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1111500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1110000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1110500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1112500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1111000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1111500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1113500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1112000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1112500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1114500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1113000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1113500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1115500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1114000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1114500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1116500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1115000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1115500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1117500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1116000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1116500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1118500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1117000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1117500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1119500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1118000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1118500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1120500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1119000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1119500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1121500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1120000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1120500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1122500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1121000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1121500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1123500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1122000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1122500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1124500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1123000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1123500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1125500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1124000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1124500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1126500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1125000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1125500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1127500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1126000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1126500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1128500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1127000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1127500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1129500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1128000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1128500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1130500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1129000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1129500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1131500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1130000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1130500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1132500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1131000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1131500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1133500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1132000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1132500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1134500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1133000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1133500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1135500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1134000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1134500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1136500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1135000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1135500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1137500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1136000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1136500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1138500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1137000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1137500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1139500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1138000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1138500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1140500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1139000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1139500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1141500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1140000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1140500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1142500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1141000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1141500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1143500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1142000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1142500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1144500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1143000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1143500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1145500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1144000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1144500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1146500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1145000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1145500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1147500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1146000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1146500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1148500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1147000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1147500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1149500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1148000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1148500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1150500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1149000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1149500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1151500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1150000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1150500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1152500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1151000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1151500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1153500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1152000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1152500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1154500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1153000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1153500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1155500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1154000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1154500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1156500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1155000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1155500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1157500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1156000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1156500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1158500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1157000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1157500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1159500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1158000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1158500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1160500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1159000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1159500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1161500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1160000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1160500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1162500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1161000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1161500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1163500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1162000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1162500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1164500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1163000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1163500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1165500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1164000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1164500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1166500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1165000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1165500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1167500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1166000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1166500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1168500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1167000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1167500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1169500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1168000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1168500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1170500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1169000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1169500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1171500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1170000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1170500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1172500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1171000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1171500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1173500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1172000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1172500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1174500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1173000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1173500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1175500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1174000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1174500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1176500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1175000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1175500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1177500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1176000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1176500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1178500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1177000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1177500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1179500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1178000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1178500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1180500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1179000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1179500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1181500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1180000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1180500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1182500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1181000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1181500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1183500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1182000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1182500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1184500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1183000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1183500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1185500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1184000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1184500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1186500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1185000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1185500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1187500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1186000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1186500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1188500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1187000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1187500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1189500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1188000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1188500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1190500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1189000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1189500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1191500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1190000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1190500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1192500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1191000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1191500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1193500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1192000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1192500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1194500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1193000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1193500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1195500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1194000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1194500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1196500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1195000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1195500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1197500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1196000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1196500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1198500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1197000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1197500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1199500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1198000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1198500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1200500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1199000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1199500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1201500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1200000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1200500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1202500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1201000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1201500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1203500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1202000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1202500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1204500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1203000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1203500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1205500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1204000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1204500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1206500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1205000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1205500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1207500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1206000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1206500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1208500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1207000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1207500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1209500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1208000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1208500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1210500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1209000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1209500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1211500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1210000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1210500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1212500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1211000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1211500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1213500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1212000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1212500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1214500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1213000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1213500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1215500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1214000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1214500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1216500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1215000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1215500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1217500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1216000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1216500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1218500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1217000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1217500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1219500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1218000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1218500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1220500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1219000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1219500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1221500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1220000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1220500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1222500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1221000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1221500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1223500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1222000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1222500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1224500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1223000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1223500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1225500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1224000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1224500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1226500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1225000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1225500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1227500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1226000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1226500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1228500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1227000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1227500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1229500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1228000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1228500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1230500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1229000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1229500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1231500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1230000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1230500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1232500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1231000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1231500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1233500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1232000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1232500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1234500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1233000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1233500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1235500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1234000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1234500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1236500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1235000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1235500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1237500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1236000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1236500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1238500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1237000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1237500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1239500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1238000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1238500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1240500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1239000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1239500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1241500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1240000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1240500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1242500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1241000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1241500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1243500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1242000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1242500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1244500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1243000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1243500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1245500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1244000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1244500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1246500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1245000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1245500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1247500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1246000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1246500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1248500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1247000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1247500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1249500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1248000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1248500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1250500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1249000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1249500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1251500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1250000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1250500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1252500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1251000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1251500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1253500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1252000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1252500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1254500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1253000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1253500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1255500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1254000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1254500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1256500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1255000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1255500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1257500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1256000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1256500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1258500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1257000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1257500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1259500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1258000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1258500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1260500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1259000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1259500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1261500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1260000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1260500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1262500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1261000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1261500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1263500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1262000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1262500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1264500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1263000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1263500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1265500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1264000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1264500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1266500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1265000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1265500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1267500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1266000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1266500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1268500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1267000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1267500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1269500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1268000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1268500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1270500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1269000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1269500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1271500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1270000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1270500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1272500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1271000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1271500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1273500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1272000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1272500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1274500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1273000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1273500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1275500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1274000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1274500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1276500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1275000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1275500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1277500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1276000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1276500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1278500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1277000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1277500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1279500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1278000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1278500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1280500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1279000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1279500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1281500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1280000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1280500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1282500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1281000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1281500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1283500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1282000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1282500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1284500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1283000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1283500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1285500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1284000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1284500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1286500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1285000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1285500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1287500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1286000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1286500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1288500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1287000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1287500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1289500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1288000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1288500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1290500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1289000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1289500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1291500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1290000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1290500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1292500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1291000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1291500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1293500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1292000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1292500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1294500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1293000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1293500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1295500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1294000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1294500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1296500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1295000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1295500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1297500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1296000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1296500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1298500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1297000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1297500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1299500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1298000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1298500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1300500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1299000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1299500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1301500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1300000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1300500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1302500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1301000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1301500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1303500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1302000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1302500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1304500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1303000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1303500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1305500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1304000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1304500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1306500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1305000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307000\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1305500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1307500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1306000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1306500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1308500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1307000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1307500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1309500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1308000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1308500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1310500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1309000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1309500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1311500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1310000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1310500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1312500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1311000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1311500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1313500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1312000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1312500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1314500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1313000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1313500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1315500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1314000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1314500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1316500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1315000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1315500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1317500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1316000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1316500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1318500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1317000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1317500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1319500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1318000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1318500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1320500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1319000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1319500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1321500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1320000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1320500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1322500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1321000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1321500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1323500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1322000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1322500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1324500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1323000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1323500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1325500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1324000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1324500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1326500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1325000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1325500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1327500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1326000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1326500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1328500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1327000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1327500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1329500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1328000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1328500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1330500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1329000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1329500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1331500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1330000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1330500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1332500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1331000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1331500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1333500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1332000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1332500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1334500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1333000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1333500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1335500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1334000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1334500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1336500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1335000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1335500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1337500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1336000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1336500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1338500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1337000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1337500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1339500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1338000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1338500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1340500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1339000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1339500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1341500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1340000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1340500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1342500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1341000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1341500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1343500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1342000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1342500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1344500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1343000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1343500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1345500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1344000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1344500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1346500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1345000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1345500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1347500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1346000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1346500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1348500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1347000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1347500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1349500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1348000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1348500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1350500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1349000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1349500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1351500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1350000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1350500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1352500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1351000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1351500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1353500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1352000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1352500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1354500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1353000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1353500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1355500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1354000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1354500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1356500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1355000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1355500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1357500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1356000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1356500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1358500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1357000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1357500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1359500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1358000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1358500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1360500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1359000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1359500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1361500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1360000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1360500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1362500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1361000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1361500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1363500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1362000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1362500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1364500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1363000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1363500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1365500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1364000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1364500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1366500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1365000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1365500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1367500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1366000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1366500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1368500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1367000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1367500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1369500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1368000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1368500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1370500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1369000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1369500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1371500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1370000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1370500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1372500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1371000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1371500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1373500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1372000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1372500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1374500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1373000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1373500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1375500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1374000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1374500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1376500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1375000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1375500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1377500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1376000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1376500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1378500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1377000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1377500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1379500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1378000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1378500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1380500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1379000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1379500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1381500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1380000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1380500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1382500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1381000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1381500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1383500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1382000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1382500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1384500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1383000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1383500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1385500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1384000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1384500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1386500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1385000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1385500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1387500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1386000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1386500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1388500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1387000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1387500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1389500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1388000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1388500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1390500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1389000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1389500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1391500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1390000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1390500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1392500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1391000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1391500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1393500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1392000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1392500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1394500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1393000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1393500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1395500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1394000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1394500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1396500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1395000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1395500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1397500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1396000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1396500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1398500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1397000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1397500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1399500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1398000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1398500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1400500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1399000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1399500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1401500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1400000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1400500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1402500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1401000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1401500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1403500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1402000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1402500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1404500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1403000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1403500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1405500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1404000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1404500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1406500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1405000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1405500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1407500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1406000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1406500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1408500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1407000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1407500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1409500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1408000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1408500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1410500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1409000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1409500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1411500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1410000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1410500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1412500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1411000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1411500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1413500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1412000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1412500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1414500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1413000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1413500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1415500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1414000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1414500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1416500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1415000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1415500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1417500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1416000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1416500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1418500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1417000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1417500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1419500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1418000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1418500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1420500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1419000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1419500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1421500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1420000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1420500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1422500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1421000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1421500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1423500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1422000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1422500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1424500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1423000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1423500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1425500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1424000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1424500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1426500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1425000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1425500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1427500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1426000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1426500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1428500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1427000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1427500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1429500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1428000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1428500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1430500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1429000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1429500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1431500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1430000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1430500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1432500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1431000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1431500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1433500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1432000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1432500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1434500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1433000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1433500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1435500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1434000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1434500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1436500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1435000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1435500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1437500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1436000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1436500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1438500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1437000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1437500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1439500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1438000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1438500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1440500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1439000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1439500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1441500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1440000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1440500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1442500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1441000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1441500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1443500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1442000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1442500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1444500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1443000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1443500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1445500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1444000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1444500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1446500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1445000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1445500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1447500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1446000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1446500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1448500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1447000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1447500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1449500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1448000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1448500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1450500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1449000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1449500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1451500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1450000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1450500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1452500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1451000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1451500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1453500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1452000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1452500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1454500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1453000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1453500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1455500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1454000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1454500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1456500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1455000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1455500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1457500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1456000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1456500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1458500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1457000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1457500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1459500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1458000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1458500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1460500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1459000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1459500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1461500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1460000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1460500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1462500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1461000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1461500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1463500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1462000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1462500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1464500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1463000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1463500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1465500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1464000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1464500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1466500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1465000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1465500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1467500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1466000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1466500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1468500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1467000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1467500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1469500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1468000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1468500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1470500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1469000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1469500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1471500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1470000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1470500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1472500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1471000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1471500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1473500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1472000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1472500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1474500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1473000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1473500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1475500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1474000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1474500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1476500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1475000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1475500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1477500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1476000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1476500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1478500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1477000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1477500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1479500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1478000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1478500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1480500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1479000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1479500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1481500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1480000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1480500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1482500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1481000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1481500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1483500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1482000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1482500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1484500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1483000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1483500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1485500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1484000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1484500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1486500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1485000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1485500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1487500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1486000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1486500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1488500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1487000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1487500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1489500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1488000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1488500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1490500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1489000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1489500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1491500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1490000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1490500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1492500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1491000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1491500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1493500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1492000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1492500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1494500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1493000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1493500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1495500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1494000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1494500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1496500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1495000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1495500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1497500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1496000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1496500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1498500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1497000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1497500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1499500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1498000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1498500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1500500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1499000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1499500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1501500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1500000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1500500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1502500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1501000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1501500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1503500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1502000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1502500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1504500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1503000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1503500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1505500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1504000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1504500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1506500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1505000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1505500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1507500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1506000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1506500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1508500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1507000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1507500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1509500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1508000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1508500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1510500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1509000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1509500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1511500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1510000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1510500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1512500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1511000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1511500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1513500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1512000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1512500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1514500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1513000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1513500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1515500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1514000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1514500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1516500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1515000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1515500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1517500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1516000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1516500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1518500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1517000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1517500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1519500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1518000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1518500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1520500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1519000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1519500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1521500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1520000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1520500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1522500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1521000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1521500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1523500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1522000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1522500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1524500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1523000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1523500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1525500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1524000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1524500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1526500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1525000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1525500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1527500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1526000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1526500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1528500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1527000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1527500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1529500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1528000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1528500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1530500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1529000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1529500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1531500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1530000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1530500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1532500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1531000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1531500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1533500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1532000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1532500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1534500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1533000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1533500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1535500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1534000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1534500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1536500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1535000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1535500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1537500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1536000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1536500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1538500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1537000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1537500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1539500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1538000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1538500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1540500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1539000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1539500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1541500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1540000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1540500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1542500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1541000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1541500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1543500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1542000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1542500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1544500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1543000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1543500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1545500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1544000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1544500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1546500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1545000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1545500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1547500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1546000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1546500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1548500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1547000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1547500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1549500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1548000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1548500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1550500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1549000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1549500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1551500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1550000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1550500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1552500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1551000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1551500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1553500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1552000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1552500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1554500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1553000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1553500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1555500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1554000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1554500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1556500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1555000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1555500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1557500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1556000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1556500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1558500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1557000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1557500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1559500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1558000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1558500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1560500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1559000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1559500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1561500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1560000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1560500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1562500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1561000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1561500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1563500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1562000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1562500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1564500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1563000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1563500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1565500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1564000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1564500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1566500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1565000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1565500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1567500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1566000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1566500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1568500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1567000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1567500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1569500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1568000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1568500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1570500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1569000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1569500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1571500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1570000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1570500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1572500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1571000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1571500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1573500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1572000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1572500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1574500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1573000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1573500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1575500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1574000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1574500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1576500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1575000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1575500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1577500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1576000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1576500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1578500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1577000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1577500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1579500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1578000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1578500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1580500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1579000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1579500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1581500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1580000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1580500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1582500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1581000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1581500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1583500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1582000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1582500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1584500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1583000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1583500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1585500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1584000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1584500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1586500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1585000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1585500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1587500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1586000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1586500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1588500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1587000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1587500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1589500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1588000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1588500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1590500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1589000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1589500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1591500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1590000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1590500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1592500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1591000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1591500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1593500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1592000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594000\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1592500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1594500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1593000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1593500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1595500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1594000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1594500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1596500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1595000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1595500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1597500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1596000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1596500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1598500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1597000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1597500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1599500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1598000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1598500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1600500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1599000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1599500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1601500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1600000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1600500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1602500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1601000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1601500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1603500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1602000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1602500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1604500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1603000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1603500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1605500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1604000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1604500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1606500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1605000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1605500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1607500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1606000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1606500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1608500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1607000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1607500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1609500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1608000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1608500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1610500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1609000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1609500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1611500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1610000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1610500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1612500\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1611000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1611500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1613500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1612000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1612500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1614500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1613000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1613500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1615500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1614000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1614500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1616500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1615000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1615500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1617500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1616000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1616500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1618500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1617000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1617500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1619500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1618000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1618500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1620500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1619000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1619500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1621500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1620000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1620500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1622500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1621000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1621500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1623500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1622000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1622500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1624500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1623000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1623500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1625500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1624000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1624500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1626500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1625000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1625500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1627500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1626000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1626500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1628500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1627000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1627500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1629500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1628000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1628500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1630500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1629000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1629500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1631500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1630000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1630500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1632500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1631000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1631500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1633500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1632000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1632500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1634500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1633000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1633500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1635500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1634000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1634500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1636500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1635000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1635500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1637500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1636000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1636500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1638500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1637000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1637500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1639500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1638000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1638500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1640500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1639000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1639500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1641500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1640000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1640500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1642500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1641000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1641500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1643500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1642000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1642500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1644500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1643000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1643500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1645500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1644000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1644500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1646500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1645000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1645500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1647500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1646000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1646500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1648500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1647000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1647500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1649500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1648000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1648500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1650500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1649000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1649500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1651500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1650000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1650500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1652500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1651000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1651500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1653500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1652000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1652500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1654500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1653000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1653500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1655500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1654000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1654500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1656500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1655000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1655500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1657500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1656000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1656500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1658500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1657000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1657500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1659500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1658000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1658500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1660500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1659000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1659500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1661500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1660000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1660500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1662500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1661000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1661500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1663500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1662000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1662500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1664500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1663000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1663500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1665500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1664000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1664500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1666500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1665000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1665500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1667500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1666000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1666500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1668500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1667000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1667500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1669500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1668000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1668500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1670500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1669000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1669500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1671500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1670000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1670500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1672500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1671000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1671500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1673500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1672000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1672500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1674500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1673000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1673500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1675500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1674000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1674500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1676500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1675000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1675500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1677500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1676000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1676500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1678500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1677000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1677500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1679500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1678000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1678500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1680500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1679000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1679500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1681500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1680000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1680500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1682500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1681000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1681500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1683500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1682000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1682500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1684500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1683000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1683500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1685500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1684000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1684500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1686500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1685000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1685500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1687500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1686000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1686500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1688500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1687000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1687500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1689500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1688000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1688500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1690500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1689000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1689500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1691500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1690000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1690500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1692500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1691000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1691500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1693500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1692000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1692500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1694500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1693000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1693500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1695500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1694000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1694500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1696500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1695000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1695500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1697500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1696000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1696500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1698500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1697000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1697500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1699500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1698000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1698500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1700500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1699000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1699500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1701500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1700000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1700500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1702500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1701000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1701500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1703500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1702000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1702500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1704500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1703000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1703500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1705500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1704000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1704500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1706500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1705000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1705500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1707500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1706000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1706500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1708500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1707000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1707500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1709500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1708000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1708500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1710500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1709000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1709500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1711500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1710000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1710500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1712500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1711000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1711500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1713500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1712000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1712500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1714500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1713000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1713500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1715500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1714000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1714500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1716500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1715000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1715500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1717500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1716000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1716500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1718500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1717000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1717500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1719500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1718000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1718500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1720500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1719000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1719500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1721500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1720000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1720500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1722500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1721000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1721500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1723500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1722000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1722500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1724500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1723000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1723500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1725500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1724000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1724500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1726500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1725000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1725500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1727500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1726000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1726500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1728500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1727000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1727500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1729500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1728000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1728500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1730500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1729000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1729500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1731500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1730000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1730500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1732500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1731000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1731500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1733500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1732000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1732500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1734500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1733000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1733500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1735500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1734000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1734500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1736500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1735000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1735500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1737500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1736000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1736500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1738500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1737000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1737500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1739500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1738000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1738500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1740500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1739000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1739500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1741500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1740000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1740500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1742500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1741000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1741500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1743500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1742000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1742500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1744500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1743000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1743500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1745500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1744000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1744500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1746500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1745000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1745500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1747500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1746000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1746500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1748500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1747000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1747500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1749500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1748000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1748500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1750500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1749000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1749500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1751500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1750000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1750500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1752500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1751000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1751500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1753500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1752000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1752500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1754500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1753000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1753500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1755500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1754000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1754500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1756500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1755000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1755500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1757500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1756000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1756500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1758500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1757000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1757500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1759500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1758000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1758500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1760500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1759000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1759500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1761500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1760000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1760500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1762500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1761000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1761500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1763500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1762000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1762500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1764500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1763000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1763500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1765500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1764000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1764500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1766500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1765000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1765500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1767500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1766000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1766500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1768500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1767000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1767500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1769500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1768000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1768500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1770500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1769000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1769500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1771500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1770000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1770500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1772500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1771000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1771500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1773500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1772000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1772500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1774500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1773000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1773500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1775500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1774000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1774500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1776500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1775000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1775500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1777500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1776000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1776500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1778500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1777000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1777500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1779500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1778000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1778500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1780500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1779000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1779500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1781500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1780000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782000\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1780500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1782500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1781000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1781500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1783500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1782000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1782500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1784500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1783000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1783500] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1785500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1784000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1784500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1786500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1785000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1785500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1787500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1786000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1786500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1788500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1787000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1787500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1789500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1788000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1788500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1790500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1789000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1789500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1791500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1790000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1790500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1792500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1791000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1791500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1793500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1792000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1792500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1794500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1793000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1793500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1795500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1794000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1794500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1796500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1795000] due to args.save_total_limit\n",
      "C:\\Users\\marek\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1795500] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797500\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797500\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797500\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797500\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1797500\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1796000] due to args.save_total_limit\n",
      "Saving model checkpoint to Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1798000\n",
      "Configuration saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1798000\\config.json\n",
      "Model weights saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1798000\\pytorch_model.bin\n",
      "tokenizer config file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1798000\\tokenizer_config.json\n",
      "Special tokens file saved in Training-Helsinki-NLP/opus-mt-en-sk\\checkpoint-1798000\\special_tokens_map.json\n",
      "Deleting older checkpoint [Training-Helsinki-NLP\\opus-mt-en-sk\\checkpoint-1796500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13936/2108036389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m                 if (\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1860\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\comet_ml\\monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                     )\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mreturn_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# Call after callbacks once we have the return value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5edf954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Training-Helsinki-NLP/opus-mt-en-sk/checkpoint-1798000\\config.json\n",
      "Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-sk\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      60024\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 60024,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 60024,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 60025\n",
      "}\n",
      "\n",
      "loading weights file Training-Helsinki-NLP/opus-mt-en-sk/checkpoint-1798000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Training-Helsinki-NLP/opus-mt-en-sk/checkpoint-1798000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "marian_checkpoint = AutoModelForSeq2SeqLM.from_pretrained(\"Training-Helsinki-NLP/opus-mt-en-sk/checkpoint-1798000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd3c3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_small = dataset['test'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a294cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'meta', 'translation'],\n",
       "        num_rows: 1593157\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'meta', 'translation'],\n",
       "        num_rows: 177018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "573e5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: meta, id, translation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 177018\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[60024,  3067,     3, ..., 60024, 60024, 60024],\n",
       "       [60024,    17, 16503, ..., 60024, 60024, 60024],\n",
       "       [60024,   919,  1565, ..., 60024, 60024, 60024],\n",
       "       ...,\n",
       "       [60024,   787,   507, ..., 60024, 60024, 60024],\n",
       "       [60024,    17,   141, ..., 60024, 60024, 60024],\n",
       "       [60024,  1091,   287, ..., 60024, 60024, 60024]], dtype=int64), label_ids=array([[ 3067,     3,     0, ..., 60024, 60024, 60024],\n",
       "       [   17, 16503,    12, ..., 60024, 60024, 60024],\n",
       "       [  418,   151,    50, ..., 60024, 60024, 60024],\n",
       "       ...,\n",
       "       [  787,   507,  3257, ..., 60024, 60024, 60024],\n",
       "       [   17,   141,  1365, ..., 60024, 60024, 60024],\n",
       "       [   17,  1091,   287, ..., 60024, 60024, 60024]], dtype=int64), metrics={'eval_loss': 2.6828773021698, 'eval_bleu': 17.0546, 'eval_gen_len': 9.941, 'eval_runtime': 25823.934, 'eval_samples_per_second': 6.855, 'eval_steps_per_second': 6.855})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(eval_small['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f93b5",
   "metadata": {},
   "source": [
    "We can see **BLEU score of 34.2418**\n",
    "\n",
    "Other metrics:\n",
    "\n",
    "'eval_gen_len': 16.85, 'eval_runtime': 12.4035, 'eval_samples_per_second': 1.612, 'eval_steps_per_second': 0.403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d90c2",
   "metadata": {},
   "source": [
    "## Quantized model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_data_collator = DataCollatorForSeq2Seq(tokenizer, model=quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83da3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "quant_trainer = Seq2SeqTrainer(\n",
    "    quantized_model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=quant_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_trainer.predict(val_small['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79c295",
   "metadata": {},
   "source": [
    "We can see **BLEU score of 34.842**\n",
    "\n",
    "Other metrics:\n",
    "\n",
    "'eval_gen_len': 16.7, 'eval_runtime': 6.1269, 'eval_samples_per_second': 3.264, 'eval_steps_per_second': 0.816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c023f3",
   "metadata": {},
   "source": [
    "## Test on whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "385bc248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7448/1289553486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"validation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer_seq2seq.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneration_max_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_beams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneration_num_beams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     def prediction_step(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2185\u001b[0m         output = eval_loop(\n\u001b[1;32m-> 2186\u001b[1;33m             \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Prediction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2187\u001b[0m         )\n\u001b[0;32m   2188\u001b[0m         \u001b[0mtotal_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_batch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2284\u001b[0m             \u001b[1;31m# Prediction step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2285\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2287\u001b[0m             \u001b[1;31m# Update containers on host\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         )\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# in case the batch is shorter than max length, the output should be padded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m             )\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1833\u001b[0m             )\n\u001b[0;32m   1834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1835\u001b[1;33m             \u001b[0mnext_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_tokens\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1836\u001b[0m             \u001b[0mnext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_tokens\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead."
     ]
    }
   ],
   "source": [
    "trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a8ada",
   "metadata": {},
   "source": [
    "We can see **BLEU score of 35.7476**\n",
    "\n",
    "Other metrics:\n",
    "\n",
    "'eval_gen_len': 14.988, **'eval_runtime': 894.6245, 'eval_samples_per_second': 2.236,** 'eval_steps_per_second': 0.559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd895105",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f4c6e",
   "metadata": {},
   "source": [
    "We can see **BLEU score of 35.7055**\n",
    "\n",
    "Other metrics:\n",
    "\n",
    "'eval_gen_len': 15.0445, **'eval_runtime': 501.4448, 'eval_samples_per_second': 3.988,** 'eval_steps_per_second': 0.997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2df90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_runtime= 501.4448\n",
    "full_runtime= 894.6245\n",
    "print(f\"Quantization evaluation is {quant_runtime/full_runtime}% of full precision time\")\n",
    "print(f\"Quantization evaluation is {full_runtime/quant_runtime}x faster than full precision time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddad4a7",
   "metadata": {},
   "source": [
    "## different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44aadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open_subtitles\", lang1=\"en\",lang2=\"sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
